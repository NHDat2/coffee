

<!DOCTYPE html>
<html lang="vi" itemscope itemtype="http://schema.org/WebPage">
  <head><script src="/coffee/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=coffee/livereload" data-no-instant defer></script>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>Self-Attention - </title>

  <meta name="description" content="
Giới Thiệu
Self-Attention

Ý Tưởng
Cơ Chế Hoạt Động

Self-Attention

Ma Trận Query, Key, Value (Q, K, V Matrix)
Tên Gọi Của Các Attention Khi Được Áp Dụng Tại Các Vị Trí Kháu Trong Transformer
Multi-Head Attention




Attention Và Self-Attention


Tài Liệu Tham Khảo


Giới Thiệu
Ta đã đi qua tổng quan về ý tưởng của transformer, ta cũng đã bắt đầu đi sâu vào trong kiến trúc để hiểu cơ chế hoạt động với việc tìm hiểu Positional Encoding."><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Coffee",
    
    "url": "http:\/\/localhost:1313\/coffee\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "http:\/\/localhost:1313\/coffee\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "http:\/\/localhost:1313\/coffee\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "http:\/\/localhost:1313\/coffee\/post\/2023-06-15-self-attention\/",
          "name": "Self attention"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : ""
  },
  "headline": "Self-Attention",
  "description" : " Giới Thiệu Self-Attention Ý Tưởng Cơ Chế Hoạt Động Self-Attention Ma Trận Query, Key, Value (Q, K, V Matrix) Tên Gọi Của Các Attention Khi Được Áp Dụng Tại Các Vị Trí Kháu Trong Transformer Multi-Head Attention Attention Và Self-Attention Tài Liệu Tham Khảo Giới Thiệu Ta đã đi qua tổng quan về ý tưởng của transformer, ta cũng đã bắt đầu đi sâu vào trong kiến trúc để hiểu cơ chế hoạt động với việc tìm hiểu Positional Encoding.\n",
  "inLanguage" : "vi",
  "wordCount":  1426 ,
  "datePublished" : "2023-06-15T00:00:00\u002b00:00",
  "dateModified" : "2023-06-15T00:00:00\u002b00:00",
  "image" : "http:\/\/localhost:1313\/coffee\/img\/avatar-icon.png",
  "keywords" : [ "Transformer, Self-Attention, Attention" ],
  "mainEntityOfPage" : "http:\/\/localhost:1313\/coffee\/post\/2023-06-15-self-attention\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "http:\/\/localhost:1313\/coffee\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "http:\/\/localhost:1313\/coffee\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>


<meta property="og:title" content="Self-Attention" />
<meta property="og:description" content="
Giới Thiệu
Self-Attention

Ý Tưởng
Cơ Chế Hoạt Động

Self-Attention

Ma Trận Query, Key, Value (Q, K, V Matrix)
Tên Gọi Của Các Attention Khi Được Áp Dụng Tại Các Vị Trí Kháu Trong Transformer
Multi-Head Attention




Attention Và Self-Attention


Tài Liệu Tham Khảo


Giới Thiệu
Ta đã đi qua tổng quan về ý tưởng của transformer, ta cũng đã bắt đầu đi sâu vào trong kiến trúc để hiểu cơ chế hoạt động với việc tìm hiểu Positional Encoding.">
<meta property="og:image" content="http://localhost:1313/coffee/img/avatar-icon.png" />
<meta property="og:url" content="http://localhost:1313/coffee/post/2023-06-15-self-attention/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Coffee" />

  <meta name="twitter:title" content="Self-Attention" />
  <meta name="twitter:description" content="
Giới Thiệu
Self-Attention

Ý Tưởng
Cơ Chế Hoạt Động

Self-Attention

Ma Trận Query, Key, Value (Q, K, V Matrix)
Tên Gọi Của Các Attention Khi Được Áp Dụng Tại Các Vị Trí Kháu Trong Transformer …">
  <meta name="twitter:image" content="http://localhost:1313/coffee/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='http://localhost:1313/coffee/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.145.0">
  <link rel="alternate" href="http://localhost:1313/coffee/index.xml" type="application/rss+xml" title="Coffee"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.6.0/css/all.css" integrity="sha384-h/hnnw1Bi4nbpD6kE7nYfCXzovi622sY5WBxww8ARKwpdLj5kUWjRuyiXaD1U2JT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="http://localhost:1313/coffee/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="http://localhost:1313/coffee/css/highlight.min.css" /><link rel="stylesheet" href="http://localhost:1313/coffee/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">

  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="http://localhost:1313/coffee/">Coffee</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="http://localhost:1313/coffee/">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="http://localhost:1313/coffee/page/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="http://localhost:1313/coffee/tags">Tags</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="Coffee" href="http://localhost:1313/coffee/">
            <img class="avatar-img" src="http://localhost:1313/coffee/img/avatar-icon.png" alt="Coffee" />
           
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>Self-Attention</h1>
              
              
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;7&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;1426&nbsp;
  
  
    
      &nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;
    
  
  
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <ul>
<li><a href="#gi%E1%BB%9Bi-thi%E1%BB%87u">Giới Thiệu</a></li>
<li><a href="#self-attention">Self-Attention</a>
<ul>
<li><a href="#%C3%BD-t%C6%B0%E1%BB%9Fng">Ý Tưởng</a></li>
<li><a href="#c%C6%A1-ch%E1%BA%BF-ho%E1%BA%A1t-%C4%91%E1%BB%99ng">Cơ Chế Hoạt Động</a>
<ul>
<li><a href="#self-attention-1">Self-Attention</a>
<ul>
<li><a href="#ma-tr%E1%BA%ADn-query-key-value-q-k-v-matrix">Ma Trận Query, Key, Value (Q, K, V Matrix)</a></li>
<li><a href="#t%C3%AAn-g%E1%BB%8Di-c%E1%BB%A7a-c%C3%A1c-attention-khi-%C4%91%C6%B0%E1%BB%A3c-%C3%A1p-d%E1%BB%A5ng-t%E1%BA%A1i-c%C3%A1c-v%E1%BB%8B-tr%C3%AD-kh%C3%A1u-trong-transformer">Tên Gọi Của Các Attention Khi Được Áp Dụng Tại Các Vị Trí Kháu Trong Transformer</a></li>
<li><a href="#multi-head-attention">Multi-Head Attention</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#attention-v%C3%A0-self-attention">Attention Và Self-Attention</a></li>
</ul>
</li>
<li><a href="#t%C3%A0i-li%E1%BB%87u-tham-kh%E1%BA%A3o">Tài Liệu Tham Khảo</a></li>
</ul>
<style>
  .img {
    width: 500px;
    height: 500px;
    display: block;
    margin-left: auto;
    margin-right: auto;

  }
  .scaleImg {
    width: 700px;
    height: 500px;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
  .imgTitle {
    text-align: center;
  }
</style>
<h1 id="giới-thiệu">Giới Thiệu</h1>
<p>Ta đã đi qua tổng quan về ý tưởng của transformer, ta cũng đã bắt đầu đi sâu vào trong kiến trúc để hiểu cơ chế hoạt động với việc tìm hiểu Positional Encoding.</p>
<p>Trong bài viết này ta sẽ đi tiếp tới cơ chế tiếp theo, có thể nói là cơ chế quan trọng nhất trong kiến trúc Transformer là Self-Attention.</p>
<p>Nói không ngoa thì Self-Attention là linh hồn của kiến trúc transformer. Mọi thứ mới mẻ trong kiến trúc transformer được công bố tại thời điểm bấy giờ được sinh ra và xoay quanh Self-Attention.</p>
<img class="img" src="http://localhost:1313/coffee/img/Transformer/SelfAttention/sa_pos_in_transformer.png"/>
<p class="imgTitle">Hình 1: Vị trí của self-attention trong kiến trúc transformer</p>
<h1 id="self-attention">Self-Attention</h1>
<h2 id="ý-tưởng">Ý Tưởng</h2>
<p>Ví dụ, khi ta có một câu đầu vào với n token \( t_{1},\ t_{2},\ t_{3},&hellip;,\ t_{n} \) . Khi đó, một cách dễ hiểu self-attention là cơ chế giúp token \( t_{i} \)  chú ý tới các token còn lại trong câu, để có thể giúp token \( t_{i} \)  nắm được mối quan hệ của nó với các token còn lại về mặt cấu trúc câu, mặt ngữ nghĩa, ..v.v. là như thế nào.</p>
<h2 id="cơ-chế-hoạt-động">Cơ Chế Hoạt Động</h2>
<p>Trong bài báo gốc, self-attention được nhóm tác giả giới thiệu với một loạt các khái niệm khác liên quan như <strong>Scaled dot-product attention</strong> hay <strong>Multi-head attention</strong>.</p>
<p>Trong đó <strong>Scaled dot-product attention</strong> là một cơ chế self-attention và <strong>Multi-head attention</strong> là việc nối nhiều <strong>scaled dot-product attention</strong> lại với nhau và đưa qua một lớp <strong>fully connected</strong>.</p>
<img class="scaleImg" src="http://localhost:1313/coffee/img/Transformer/SelfAttention/sa_architecture.png"/>
<p class="imgTitle">Hình 2: Self-Attention trong transformer</p>
<h3 id="self-attention-1">Self-Attention</h3>
<h4 id="ma-trận-query-key-value-q-k-v-matrix">Ma Trận Query, Key, Value (Q, K, V Matrix)</h4>
<p>Trên Hình 2, có thể thấy Q, K, V cũng là 3 tham số được giới thiệu trong self-attention. Vậy Q, K, V là gì và đóng vai trò như thế nào trong self-attention.</p>
<p>Q, K, V là 3 vector đại diện biểu diễn cho từng token trong câu được tạo ra bằng cách nhân ma trận biểu diễn các token đầu vào với 3 ma trận học tương ứng là \( W^{Q}\ W^{K}\ W^{V} \) .</p>
<img class="scaleImg" src="http://localhost:1313/coffee/img/Transformer/SelfAttention/sa_qkv.jpg">
<p class="imgTitle">Hình 3: Ma trận Q, K, V trong self-attention</p>
<p>Trong đó:</p>
<ul>
<li><strong>Q</strong>: Query vector dùng để chứa thông tin của câu tìm kiếm (ví dụ như chứa các thông tin của token đang cần xem xét).</li>
<li><strong>K</strong>: Key vector dùng để biểu diễn thông tin so sánh giữa các token trong câu với token đang được query.</li>
<li><strong>V</strong>: value vector biểu diễn nội dung của các token.</li>
</ul>
<p>Để dễ hiểu hơn, nếu câu đầu vào là <strong>&ldquo;tôi đi học&rdquo;</strong> với số chiều embedding là \( d=100 \)  thì <strong>Q, K, V</strong> sẽ được biểu diễn dưới dạng ma trận có \( (3 \times 100) \) .</p>
<p>Khi đó, ma trận \( attention \_ score = softmax(\frac{QK^{T}}{\sqrt[2]d_{k}}) \)  có thể được biểu diễn dưới dạng visulaize:</p>
<img class="img" src="http://localhost:1313/coffee/img/Transformer/SelfAttention/attention_score.png">
<p class="imgTitle">Hình 4: Visualize cách thức ma trận attention_score biểu diễn mối quan hệ giữa các token trong câu</p>
<p>Có thể thấy việc thực hiện \( attention \_ score = softmax(\frac{QK^{T}}{\sqrt[2]d_{k}}) \)  sẽ giúp cho mô hình có thể học được mối quan hệ của các từ trong câu, như (&ldquo;tôi-tôi&rdquo;, &ldquo;tôi-đi&rdquo;, &ldquo;tôi-học&rdquo;, &ldquo;đi-tôi&rdquo;, &ldquo;đi-đi&rdquo;, &ldquo;đi-học&rdquo; ..v.v..).</p>
<p>Tuy nhiên, với đó thì chưa đủ vì bản chất đó chỉ là học các mối liên hệ giữa các token trong câu nhưng không giữ được giá trị, ý nghĩa của cả câu ban đầu là <strong>&ldquo;tôi đi học&rdquo;</strong> mang giá trị gì.</p>
<p>Thì khi đó <strong>V</strong> ở đây để giữ nguyên giá trị, ý nghĩa của câu đầu vào đó để kết hợp với attention_score và tạo thành một biểu thức self-attention hoàn chỉnh, thứ mà giúp model vừa có thể hiểu giá trị, ý nghĩa tổng quan của cả câu đầu vào vừa có thể hiểu mối quan hệ giữa các token trong câu với nhau.</p>
<p>Khi đó, ta có công thức cho attention là:</p>
<p>\[ Self - Attention \ Output(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt[2]d_{k}})V \]</p>
<p>Đây được coi là hàm số để tính điểm cho attention, tham số \( \sqrt[2]d_{k} \)  xuất hiện ở đây với mục đích scale nhỏ lại bộ giá trị ở tử số trong hàm softmax.</p>
<p>Nếu giá trị \( QK^{T} \)  là một vector lớn và không chia cho tham số \( \sqrt[2]d_{k} \) , thì khi đó với tính chất của hàm mũ trong hàm softmax là \( \frac{e^{z_{i}}}{\sum_{j}^{nclass}e^{z_{j}}} \)  thì input càng lớn sẽ càng khiến giá trị lớn nhất trong input tiến tới 1 và các giá trị còn lại tiến dần tới 0.</p>
<p>Ví dụ, nếu như ta có một hàm softmax cho 5 class với input bất kỳ, ta thực hiện scale từ nhỏ đến lớn thì khi đó đồ thị phân bố xác suất cho hàm softmax đó có dạng:</p>
<img class="img" src="http://localhost:1313/coffee/img/Transformer/SelfAttention/scale_input_softmax.png">
<p class="imgTitle">Hình 5: Đồ thị phân bố xác suất của hàm softmax khi scale input</p>
<p>Thì khi đó các class có giá trị sau khi đi qua hàm softmax tiến tới 0 khi train trong quá trình backpropagation sẽ xảy ra hiện tượng <strong>vanishing gradient</strong> và sẽ không đóng góp gì nhiều giá trị học trong quá trình train.</p>
<p>Do vậy, nhóm tác giả thực hiện scale nhỏ lại input của hàm softmax bằng tham số \( \sqrt[2]d_{k} \) để giúp cho các class khác mặc dù vẫn sẽ thấp nhưng không bị thấp quá.</p>
<h4 id="encoder-decoder-cross-attention">Encoder, Decoder, Cross Attention</h4>
<img class="img" src="http://localhost:1313/coffee/img/Transformer/SelfAttention/sa_type.png">
<p class="imgTitle">Hình 6: Tên gọi của các attention khi được áp dụng tại các vị trí khác nhau</p>
<p>Về mặt bản chất các Attention trong Transformer có chung một cơ chế là Self-Attention như ở phần trước.</p>
<p>Tuy Nhiên, khác với encoder và decoder attention nhận đầu vào là các represent vector của câu đầu vào được đi qua embedding layer và cộng với position vector, thì cross attention layer nhận đầu vào từ encoder và decoder để học mối quan hệ giữa 2 phần.</p>
<h4 id="multi-head-attention">Multi-Head Attention</h4>
<img class="scaleImg" src="http://localhost:1313/coffee/img/Transformer/SelfAttention/sa_qkv_multi.jpg">
<p class="imgTitle">Hình 7: Ma trận Q, K, V trong multi-head attention</p>
<p>Về cơ bản Multi-head attention có thể được định nghĩa là việc sử dụng nhiều lớp self-attention rồi nối chúng lại với nhau, sau đó nhân với một ma trận học \( W^{O} \)</p>
<p>Thông thường, để hiểu được vai trò của một từ trong câu, nó sẽ cần được nhìn ở nhiều khía cạnh khác nhau như cấu trúc câu, ngữ nghĩa, ..v.v.</p>
<p>Thì thay vì chỉ sử dụng 1 self-attention hay còn gọi là 1 head thì nhóm tác giả sử dụng nhiều self-attention hay multi-head để mỗi head sẽ tập trung vào học một khía cạnh khác nhau.</p>
<p><strong>Note</strong>: Thứ mà model học là các ma trận \( W^{Q},\ W^{K},\ W^{V},\ W^{O}\) Chứ <strong>không phải</strong> các ma trận <strong>Q, K, V, Z</strong>. Đừng nhầm lẫn nhé</p>
<h2 id="attention-và-self-attention">Attention Và Self-Attention</h2>
<p>Trước đó, với mô hình seq2seq, ở đâu đó ta đã biết qua một số cơ chế <strong>attention</strong> khác nhau để giúp seq2seq model có sự chú ý tới các token trong câu có thể đến như <strong>Bahdanau</strong> và <strong>Luong Attention</strong>.</p>
<p>Sau đây ta sẽ đi qua một nhận định giữa Attention và Self-Attention theo góc nhìn và hiểu biết của mình.</p>
<ul>
<li>
<p>Với các neural network ta sẽ đưa input (câu đầu vào) qua các layer và các activate fuction, và trong các mạng RNN và biến thể của nó thì ta sẽ có thêm state của các layer nữa. Thì khi đó Attention được áp dụng sẽ nhận đầu vào là các input đã đi qua các layer và các activate fuction. Trong khi đó Self-Attention sẽ thực hiện attention tại chính câu đầu vào ở mỗi layer có sử dụng nó.</p>
</li>
<li>
<p>Attention thường được áp dụng để giúp bộ phận decoder có thể có thêm thông tin về phía encoder. Self-Attention có thể hoạt động độc lập trên cả 2 bộ phân encoder và decoder mà không có sự kết nối nào ở đây. Do hoạt động độc lập nên có cả các biến thể của Transformer được sinh ra khi chỉ dùng Encoder hoặc Decoder (BERT là một ví dụ điển hình).</p>
</li>
</ul>
<h1 id="tài-liệu-tham-khảo">Tài Liệu Tham Khảo</h1>
<p>[1] <a href="https://arxiv.org/abs/1706.03762">Ashish Vaswani et al, “Attention Is All You Need”, NeurIPS 2017</a></p>
<p>[2] <a href="https://theaisummer.com/self-attention/">Why multi-head self attention works: math, intuitions and 10+1 hidden insights - Nikolas Adaloglou</a></p>
<p>[3] <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer - Jay Alammar</a></p>
<p>[4] <a href="https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500">Transformer Networks: A mathematical explanation why scaling the dot products leads to more stable gradients</a></p>


        
          <div class="blog-tags">
            
              
              <a href="http://localhost:1313/coffee/tags/transformer/">Transformer</a>&nbsp;
            
              
              <a href="http://localhost:1313/coffee/tags/self-attention/">Self-Attention</a>&nbsp;
            
              
              <a href="http://localhost:1313/coffee/tags/attention/">Attention</a>&nbsp;
            
          </div>
        

        

        
          
            
          

          
                  <h4 class="see-also"></h4>
                  <ul>
                
                
                    <li><a href="http://localhost:1313/coffee/post/2023-06-20-bert/">BERT</a></li>
                
                    <li><a href="http://localhost:1313/coffee/post/2023-06-16-transformer-rest/">The Rest of Transformer</a></li>
                
                    <li><a href="http://localhost:1313/coffee/post/2023-06-14-positional-encoder/">Positional Encoding</a></li>
                
              </ul>

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="http://localhost:1313/coffee/post/2023-06-14-positional-encoder/" data-toggle="tooltip" data-placement="top" title="Positional Encoding">&larr; </a>
            </li>
          
          
            <li class="next">
              <a href="http://localhost:1313/coffee/post/2023-06-16-transformer-rest/" data-toggle="tooltip" data-placement="top" title="The Rest of Transformer"> &rarr;</a>
            </li>
          
        </ul>
      


      
      
      
      
      
        
      

    </div>
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          
          
        </ul>
        <p class="credits copyright text-muted">
          

          &nbsp;&bull;&nbsp;&copy;
          
            2023
          

          
            &nbsp;&bull;&nbsp;
            <a href="http://localhost:1313/coffee/">Coffee</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="http://localhost:1313/coffee/js/main.js"></script>
<script src="http://localhost:1313/coffee/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="http://localhost:1313/coffee/js/load-photoswipe.js"></script>










    
  </body>
</html>

