

<!DOCTYPE html>
<html lang="vi" itemscope itemtype="http://schema.org/WebPage">
  <head><script src="/coffee/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=coffee/livereload" data-no-instant defer></script>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>The Rest of Transformer - </title>

  <meta name="description" content="
Giới Thiệu
Residual Connections And Feed Forward Layer
Một Số Nhận Định Và Các Nghiên Cứu Liên Quan

Cắt Tỉa Multi-Head Attention Với Encoder Attention
Multi-Head Cực Kỳ Quan Trong Đối Với Cross Attention (Encoder-Decoder Attention)
Tài Liệu Tham Khảo




Giới Thiệu
Ở các phần trước đó ta đã đi qua các phần kiến thức trọng yếu trong kiến trúc Transformer."><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Coffee",
    
    "url": "http:\/\/localhost:1313\/coffee\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "http:\/\/localhost:1313\/coffee\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "http:\/\/localhost:1313\/coffee\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "http:\/\/localhost:1313\/coffee\/post\/2023-06-16-transformer-rest\/",
          "name": "The rest of transformer"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : ""
  },
  "headline": "The Rest of Transformer",
  "description" : " Giới Thiệu Residual Connections And Feed Forward Layer Một Số Nhận Định Và Các Nghiên Cứu Liên Quan Cắt Tỉa Multi-Head Attention Với Encoder Attention Multi-Head Cực Kỳ Quan Trong Đối Với Cross Attention (Encoder-Decoder Attention) Tài Liệu Tham Khảo Giới Thiệu Ở các phần trước đó ta đã đi qua các phần kiến thức trọng yếu trong kiến trúc Transformer.\n",
  "inLanguage" : "vi",
  "wordCount":  613 ,
  "datePublished" : "2023-06-16T00:00:00\u002b00:00",
  "dateModified" : "2023-06-16T00:00:00\u002b00:00",
  "image" : "http:\/\/localhost:1313\/coffee\/img\/avatar-icon.png",
  "keywords" : [ "Transformer, Residual, Feed Forward" ],
  "mainEntityOfPage" : "http:\/\/localhost:1313\/coffee\/post\/2023-06-16-transformer-rest\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "http:\/\/localhost:1313\/coffee\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "http:\/\/localhost:1313\/coffee\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>


<meta property="og:title" content="The Rest of Transformer" />
<meta property="og:description" content="
Giới Thiệu
Residual Connections And Feed Forward Layer
Một Số Nhận Định Và Các Nghiên Cứu Liên Quan

Cắt Tỉa Multi-Head Attention Với Encoder Attention
Multi-Head Cực Kỳ Quan Trong Đối Với Cross Attention (Encoder-Decoder Attention)
Tài Liệu Tham Khảo




Giới Thiệu
Ở các phần trước đó ta đã đi qua các phần kiến thức trọng yếu trong kiến trúc Transformer.">
<meta property="og:image" content="http://localhost:1313/coffee/img/avatar-icon.png" />
<meta property="og:url" content="http://localhost:1313/coffee/post/2023-06-16-transformer-rest/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Coffee" />

  <meta name="twitter:title" content="The Rest of Transformer" />
  <meta name="twitter:description" content="
Giới Thiệu
Residual Connections And Feed Forward Layer
Một Số Nhận Định Và Các Nghiên Cứu Liên Quan

Cắt Tỉa Multi-Head Attention Với Encoder Attention
Multi-Head Cực Kỳ Quan Trong Đối Với Cross …">
  <meta name="twitter:image" content="http://localhost:1313/coffee/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='http://localhost:1313/coffee/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.145.0">
  <link rel="alternate" href="http://localhost:1313/coffee/index.xml" type="application/rss+xml" title="Coffee"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.6.0/css/all.css" integrity="sha384-h/hnnw1Bi4nbpD6kE7nYfCXzovi622sY5WBxww8ARKwpdLj5kUWjRuyiXaD1U2JT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="http://localhost:1313/coffee/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="http://localhost:1313/coffee/css/highlight.min.css" /><link rel="stylesheet" href="http://localhost:1313/coffee/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">

  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="http://localhost:1313/coffee/">Coffee</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="http://localhost:1313/coffee/">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="http://localhost:1313/coffee/page/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="http://localhost:1313/coffee/tags">Tags</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="Coffee" href="http://localhost:1313/coffee/">
            <img class="avatar-img" src="http://localhost:1313/coffee/img/avatar-icon.png" alt="Coffee" />
           
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>The Rest of Transformer</h1>
              
              
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;3&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;613&nbsp;
  
  
    
      &nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;
    
  
  
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <ul>
<li><a href="#gi%E1%BB%9Bi-thi%E1%BB%87u">Giới Thiệu</a></li>
<li><a href="#residual-connections-and-feed-forward-layer">Residual Connections And Feed Forward Layer</a></li>
<li><a href="#m%E1%BB%99t-s%E1%BB%91-nh%E1%BA%ADn-%C4%91%E1%BB%8Bnh-v%C3%A0-c%C3%A1c-nghi%C3%AAn-c%E1%BB%A9u-li%C3%AAn-quan">Một Số Nhận Định Và Các Nghiên Cứu Liên Quan</a>
<ul>
<li><a href="#c%E1%BA%AFt-t%E1%BB%89a-multi-head-attention-v%E1%BB%9Bi-encoder-attention">Cắt Tỉa Multi-Head Attention Với Encoder Attention</a></li>
<li><a href="#multi-head-c%E1%BB%B1c-k%E1%BB%B3-quan-trong-%C4%91%E1%BB%91i-v%E1%BB%9Bi-cross-attention-encoder-decoder-attention">Multi-Head Cực Kỳ Quan Trong Đối Với Cross Attention (Encoder-Decoder Attention)</a></li>
<li><a href="#t%C3%A0i-li%E1%BB%87u-tham-kh%E1%BA%A3o">Tài Liệu Tham Khảo</a></li>
</ul>
</li>
</ul>
<style>
  #imgResidual {
    width: 700px;
    height: 300px;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
  #imgFFW {
    width: 300px;
    height: 500px;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
  .imgTitle {
    text-align: center;
  }
</style>
<h1 id="giới-thiệu">Giới Thiệu</h1>
<p>Ở các phần trước đó ta đã đi qua các phần kiến thức trọng yếu trong kiến trúc Transformer.</p>
<p>Trong phần này, ta sẽ cùng nhau tìm hiểu nốt một số kiến thức nhỏ khác được thêm vào trong transformer như các phụ gia trong món ăn và cùng nhau đi qua một số nhận định mà mình survey được khi tìm hiểu về Transformer.</p>
<h1 id="residual-connections-and-feed-forward-layer">Residual Connections And Feed Forward Layer</h1>
<p>Vector đầu ra của Multi-Head Attention được cộng thêm với positional embedding, được gọi là residual connection.</p>
<p>Và sau đó đầu ra của residual connection sẽ được chuẩn hóa thông qua layer Normalization rồi được đưa qua lớp Feed Forward. Bao gồm 2 linear layer và kẹp ở giữa là một activate function RELU.</p>
<img id="imgResidual" src="http://localhost:1313/coffee/img/Transformer/Rest/residual.png">
<p class="imgTitle">Hình 1: Residual được Normalize</p>
<p>Residual connections và feed forward là 2 thành phần nhỏ nhưng góp phần quan trọng trong quá trình train của model.</p>
<p>Như đã biết, self-attention là cơ chế giúp các token nắm bắt mối quan hệ của chính nó với các token khác trong câu.</p>
<p>Tuy nhiên, ở đó thì self-attention không chứa các thông tin về vị trí và cho phép luồng thông tin tùy ý đi qua mạng.</p>
<p>Do vậy, residual connection đóng một vai trò để luôn nhắc nhở cho self-attention rằng &ldquo;các token được biểu diễn theo thứ tự như vậy là nó có ý nghĩa của nó đấy nên đừng có đảo loạn các token lên nhé&rdquo;.</p>
<img id="imgFFW" src="http://localhost:1313/coffee/img/Transformer/Rest/ffw.png">
<p class="imgTitle">Hình 2: Feed Forward layer</p>
<p>Sau đó, đầu ra của residual connection được đưa vào layer feed forward với các kết nối đầy đủ để mang lại khả năng biểu diễn phong phú hơn cho các features với 2 lớp linear và kẹp ở giữa là hàm relu.</p>
<p>Để có thể ổn định mạng, trước khi đưa vào layer feed forward, đầu ra của residual được chuẩn hóa thông qua layer normalization.</p>
<h1 id="một-số-nhận-định-và-các-nghiên-cứu-liên-quan">Một Số Nhận Định Và Các Nghiên Cứu Liên Quan</h1>
<h2 id="cắt-tỉa-multi-head-attention-với-encoder-attention">Cắt Tỉa Multi-Head Attention Với Encoder Attention</h2>
<p>Trong paper [3], nhóm tác giả đã chỉ ra trong Multi-Head attention có 3 loại heads chính:</p>
<ul>
<li>Positional heads, chú tâm vào mối quan hệ vị trí các token trong câu.</li>
<li>Syntactic heads, chú tâm vào mối quan hệ cú pháp trong câu.</li>
<li>Rare Words heads, chú tâm vào các từ hiếm trong câu.</li>
</ul>
<p>Nhóm tác giả cũng đã thực hiện thử nghiệm cắt tỉa các heads trong Encoder trên tác vụ machine translation với 2 bộ dữ liệu &ldquo;WMT&rdquo; và &ldquo;OpenSubtitles&rdquo;</p>
<img src="http://localhost:1313/coffee/img/Transformer/Rest/encoder_prune.png">
<p class="imgTitle">Hình 3: Cắt tỉa heads trong Encoder Attention [3]</p>
<h2 id="multi-head-cực-kỳ-quan-trong-đối-với-cross-attention-encoder-decoder-attention">Multi-Head Cực Kỳ Quan Trong Đối Với Cross Attention (Encoder-Decoder Attention)</h2>
<p>Trong paper [4], nhóm tác giả đã chỉ ra tầm quan trọng của multi-head đối với Encoder-Decoder Attention. Nhóm tác giả thực hiện cắt giảm dần các heads trong cross attention layer trong tác vụ machine translation và nhận thấy sự sụt giảm mạnh độ chính xác của model.</p>
<img src="http://localhost:1313/coffee/img/Transformer/Rest/cross_attention_prune.png">
<p class="imgTitle">Hình 4: Cắt tỉa heads trong Cross Attention [4]</p>
<h2 id="tài-liệu-tham-khảo">Tài Liệu Tham Khảo</h2>
<p>[1] <a href="https://arxiv.org/abs/1706.03762">Ashish Vaswani et al, “Attention Is All You Need”, NeurIPS 2017</a></p>
<p>[2] <a href="https://theaisummer.com/self-attention/">Why multi-head self attention works: math, intuitions and 10+1 hidden insights - Nikolas Adaloglou</a></p>
<p>[3] <a href="https://arxiv.org/abs/1905.09418">Voita, E., Talbot, D., Moiseev, F., Sennrich, R., &amp; Titov, I. (2019). Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418.</a></p>
<p>[4] <a href="https://arxiv.org/abs/1905.10650">Michel, P., Levy, O., &amp; Neubig, G. (2019). Are sixteen heads really better than one?. arXiv preprint arXiv:1905.10650.</a></p>


        
          <div class="blog-tags">
            
              
              <a href="http://localhost:1313/coffee/tags/transformer/">Transformer</a>&nbsp;
            
              
              <a href="http://localhost:1313/coffee/tags/residual/">Residual</a>&nbsp;
            
              
              <a href="http://localhost:1313/coffee/tags/feed-forward/">Feed Forward</a>&nbsp;
            
          </div>
        

        

        
          
            
          

          
                  <h4 class="see-also"></h4>
                  <ul>
                
                
                    <li><a href="http://localhost:1313/coffee/post/2023-06-20-bert/">BERT</a></li>
                
                    <li><a href="http://localhost:1313/coffee/post/2023-06-15-self-attention/">Self-Attention</a></li>
                
                    <li><a href="http://localhost:1313/coffee/post/2023-06-14-positional-encoder/">Positional Encoding</a></li>
                
              </ul>

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="http://localhost:1313/coffee/post/2023-06-15-self-attention/" data-toggle="tooltip" data-placement="top" title="Self-Attention">&larr; </a>
            </li>
          
          
            <li class="next">
              <a href="http://localhost:1313/coffee/post/2023-06-17-pretrain/" data-toggle="tooltip" data-placement="top" title="Tản Mạn Pre-Trained Models"> &rarr;</a>
            </li>
          
        </ul>
      


      
      
      
      
      
        
      

    </div>
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          
          
        </ul>
        <p class="credits copyright text-muted">
          

          &nbsp;&bull;&nbsp;&copy;
          
            2023
          

          
            &nbsp;&bull;&nbsp;
            <a href="http://localhost:1313/coffee/">Coffee</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="http://localhost:1313/coffee/js/main.js"></script>
<script src="http://localhost:1313/coffee/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="http://localhost:1313/coffee/js/load-photoswipe.js"></script>










    
  </body>
</html>

