<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Coffee</title>
    <link>http://localhost:1313/coffee/</link>
    <description>Recent content on Coffee</description>
    <generator>Hugo</generator>
    <language>vi</language>
    <lastBuildDate>Sat, 16 Sep 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/coffee/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ensemble Theory - Gradient Boosting (Idea)</title>
      <link>http://localhost:1313/coffee/post/2023-09-16-gbm-idea/</link>
      <pubDate>Sat, 16 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-09-16-gbm-idea/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    margin-left: 10px;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Boosting là một loại kỹ thuật học máy, trong khuân khổ “Ensemble Learning” cùng với các kỹ thuật khác như “Bagging”, “Stacking”, ..v.v&lt;/p&gt;&#xA;&lt;p&gt;Ta có thể thấy, việc build ra các model với độ chính xác cao cho các bài toán là cả một vấn đề, nhưng ngược trở lại, để có được các model đơn giản với độ chính xác ở mức trung bình thì lại không phải điều gì khó khăn.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dealing With Imbalanced Datasets</title>
      <link>http://localhost:1313/coffee/post/2023-09-15-deal-imbalanced-data/</link>
      <pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-09-15-deal-imbalanced-data/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    margin-left: 10px;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới Thiệu&lt;/h1&gt;&#xA;&lt;p&gt;Imbalanced Datasets là vấn đề mà ta thường gặp phải khi giải quyết các bài toán với dữ liệu thực tế. Và nó luôn là một trong những vấn đề khó nhằn khi giải quyết các bài toán machine learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Active Learning - Uncertainty-based Sampling</title>
      <link>http://localhost:1313/coffee/post/2023-09-13-active-learning-uncertainty/</link>
      <pubDate>Wed, 13 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-09-13-active-learning-uncertainty/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 500px;&#xA;    height: 500px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới Thiệu&lt;/h1&gt;&#xA;&lt;p&gt;Active learning, là một trong những cách “learn” trong “machine learning” =)).&lt;/p&gt;&#xA;&lt;p&gt;Thường được áp dụng cho các bài toán “supervised learning”, mà ở đó, ta gặp khó khăn với “nhãn (label)” của dữ liệu.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Activation Function</title>
      <link>http://localhost:1313/coffee/post/2023-09-10-activation-function/</link>
      <pubDate>Sun, 10 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-09-10-activation-function/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới Thiệu&lt;/h1&gt;&#xA;&lt;p&gt;Activation function, ta thường được biết đến là các hàm “phi tuyến (non-linear)” được dùng “phổ biến” trong các Neural Network models, với mục đích là bẻ cong sự “tuyến tính” của các hàm số, của models, giúp cho models có thể học được các pattern phi tuyến.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tổng Quan Self-Supervised Learning</title>
      <link>http://localhost:1313/coffee/post/2023-08-29-ssl/</link>
      <pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-08-29-ssl/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới Thiệu&lt;/h1&gt;&#xA;&lt;p&gt;Theo những gì mình research được, thì Self-Supervised Learning được đề cập và giới thiệu lần đầu trong ngành robotics.&lt;/p&gt;&#xA;&lt;p&gt;Sau đó, các cộng đồng về AI nói chung và machine learning nói riêng đã tìm hiểu cũng như phát triển thêm về idea đó.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational AutoEncoder (VAE)</title>
      <link>http://localhost:1313/coffee/post/2023-08-24-vae/</link>
      <pubDate>Thu, 24 Aug 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-08-24-vae/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới Thiệu&lt;/h1&gt;&#xA;&lt;p&gt;Variational AutoEncoder (VAE) là một biến thể của AutoEncoder models.&lt;/p&gt;&#xA;&lt;p&gt;VAE là một loại “Generative Models” thuộc nhánh “Explicit Density Generative Models”.&lt;/p&gt;&#xA;&lt;p&gt;Trong bài viết này, ta sẽ cùng đi tìm hiểu xem VAE hoạt động như nào nhé.&lt;/p&gt;</description>
    </item>
    <item>
      <title>BERT</title>
      <link>http://localhost:1313/coffee/post/2023-06-20-bert/</link>
      <pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-20-bert/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;mối-liên-hệ-của-bert-với-transformer&#34;&gt;Mối liên hệ của Bert với Transformer&lt;/h1&gt;&#xA;&lt;p&gt;BERT, cũng là một trong số các pre-trained models dựa vào kiến trúc Transformer&lt;/p&gt;&#xA;&lt;p&gt;Tuy nhiên, chỉ là kế thừa và dựa trên kiến trúc Transformer nên BERT cũng có những điểm khác biệt có thể kể đến như sau:&lt;/p&gt;</description>
    </item>
    <item>
      <title>AutoRegressive Model</title>
      <link>http://localhost:1313/coffee/post/2023-06-19-auto-regression/</link>
      <pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-19-auto-regression/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới Thiệu&lt;/h1&gt;&#xA;&lt;p&gt;Auto-regressive model là một loại mô hình khá quan trọng và được sử dụng trong nhiều tác vụ khác nhau.&lt;/p&gt;&#xA;&lt;p&gt;Đồng thời nó cũng là nền móng của rất nhiều “mô hình sinh (Generative Model)”, có thể kể đến như “NADE, MADE, PixelRNN, PixelCNN, hay họ GPT, ..v.v.”.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AutoEncoder Model</title>
      <link>http://localhost:1313/coffee/post/2023-06-18-auto-encoder/</link>
      <pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-18-auto-encoder/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới Thiệu&lt;/h1&gt;&#xA;&lt;p&gt;AutoEncoder truyền thống là một dạng của Neural-Network được thiết kế để có khả năng học biểu diễn một cách hiệu quả của dữ liệu đầu vào mà không cần nhãn (unsupervised learning).&lt;/p&gt;&#xA;&lt;p&gt;AutoEncoder architecture truyền thống là một trường hợp đặc biệt của “Encoder-Decoder Architecture” khi mà ở đó “input” và “output” của AutoEncoder là giống nhau.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tản Mạn Pre-Trained Models</title>
      <link>http://localhost:1313/coffee/post/2023-06-17-pretrain/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-17-pretrain/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;transformer-based-pre-trained-models-là-gì-&#34;&gt;Transformer-based Pre-trained Models là gì ?&lt;/h1&gt;&#xA;&lt;p&gt;T-PTMs là các model lấy kiến trúc Transformer đình đám được công bố vào năm 2018 làm nền tảng phát triển với các biến thể khác nhau cùng với lượng cực lớn dữ liệu để giúp model có thể nắm bắt được 1 khối kiến thức đồ sộ.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Rest of Transformer</title>
      <link>http://localhost:1313/coffee/post/2023-06-16-transformer-rest/</link>
      <pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-16-transformer-rest/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gi%E1%BB%9Bi-thi%E1%BB%87u&#34;&gt;Giới Thiệu&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#residual-connections-and-feed-forward-layer&#34;&gt;Residual Connections And Feed Forward Layer&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#m%E1%BB%99t-s%E1%BB%91-nh%E1%BA%ADn-%C4%91%E1%BB%8Bnh-v%C3%A0-c%C3%A1c-nghi%C3%AAn-c%E1%BB%A9u-li%C3%AAn-quan&#34;&gt;Một Số Nhận Định Và Các Nghiên Cứu Liên Quan&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#c%E1%BA%AFt-t%E1%BB%89a-multi-head-attention-v%E1%BB%9Bi-encoder-attention&#34;&gt;Cắt Tỉa Multi-Head Attention Với Encoder Attention&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#multi-head-c%E1%BB%B1c-k%E1%BB%B3-quan-trong-%C4%91%E1%BB%91i-v%E1%BB%9Bi-cross-attention-encoder-decoder-attention&#34;&gt;Multi-Head Cực Kỳ Quan Trong Đối Với Cross Attention (Encoder-Decoder Attention)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#t%C3%A0i-li%E1%BB%87u-tham-kh%E1%BA%A3o&#34;&gt;Tài Liệu Tham Khảo&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;style&gt;&#xA;  #imgResidual {&#xA;    width: 700px;&#xA;    height: 300px;&#xA;    display: block;&#xA;    margin-left: auto;&#xA;    margin-right: auto;&#xA;  }&#xA;  #imgFFW {&#xA;    width: 300px;&#xA;    height: 500px;&#xA;    display: block;&#xA;    margin-left: auto;&#xA;    margin-right: auto;&#xA;  }&#xA;  .imgTitle {&#xA;    text-align: center;&#xA;  }&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới Thiệu&lt;/h1&gt;&#xA;&lt;p&gt;Ở các phần trước đó ta đã đi qua các phần kiến thức trọng yếu trong kiến trúc Transformer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Self-Attention</title>
      <link>http://localhost:1313/coffee/post/2023-06-15-self-attention/</link>
      <pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-15-self-attention/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gi%E1%BB%9Bi-thi%E1%BB%87u&#34;&gt;Giới Thiệu&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#self-attention&#34;&gt;Self-Attention&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#%C3%BD-t%C6%B0%E1%BB%9Fng&#34;&gt;Ý Tưởng&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#c%C6%A1-ch%E1%BA%BF-ho%E1%BA%A1t-%C4%91%E1%BB%99ng&#34;&gt;Cơ Chế Hoạt Động&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#self-attention-1&#34;&gt;Self-Attention&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#ma-tr%E1%BA%ADn-query-key-value-q-k-v-matrix&#34;&gt;Ma Trận Query, Key, Value (Q, K, V Matrix)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#t%C3%AAn-g%E1%BB%8Di-c%E1%BB%A7a-c%C3%A1c-attention-khi-%C4%91%C6%B0%E1%BB%A3c-%C3%A1p-d%E1%BB%A5ng-t%E1%BA%A1i-c%C3%A1c-v%E1%BB%8B-tr%C3%AD-kh%C3%A1u-trong-transformer&#34;&gt;Tên Gọi Của Các Attention Khi Được Áp Dụng Tại Các Vị Trí Kháu Trong Transformer&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#multi-head-attention&#34;&gt;Multi-Head Attention&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#attention-v%C3%A0-self-attention&#34;&gt;Attention Và Self-Attention&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#t%C3%A0i-li%E1%BB%87u-tham-kh%E1%BA%A3o&#34;&gt;Tài Liệu Tham Khảo&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;style&gt;&#xA;  .img {&#xA;    width: 500px;&#xA;    height: 500px;&#xA;    display: block;&#xA;    margin-left: auto;&#xA;    margin-right: auto;&#xA;&#xA;  }&#xA;  .scaleImg {&#xA;    width: 700px;&#xA;    height: 500px;&#xA;    display: block;&#xA;    margin-left: auto;&#xA;    margin-right: auto;&#xA;  }&#xA;  .imgTitle {&#xA;    text-align: center;&#xA;  }&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới Thiệu&lt;/h1&gt;&#xA;&lt;p&gt;Ta đã đi qua tổng quan về ý tưởng của transformer, ta cũng đã bắt đầu đi sâu vào trong kiến trúc để hiểu cơ chế hoạt động với việc tìm hiểu Positional Encoding.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Positional Encoding</title>
      <link>http://localhost:1313/coffee/post/2023-06-14-positional-encoder/</link>
      <pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-14-positional-encoder/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gi%E1%BB%9Bi-thi%E1%BB%87u&#34;&gt;Giới Thiệu&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#positional-encoding&#34;&gt;Positional Encoding&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#t%E1%BA%A1i-sao-c%E1%BA%A7n-c%C3%B3-positional-encoding-&#34;&gt;Tại Sao Cần Có Positional Encoding ?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#kh%C3%A1i-ni%E1%BB%87m&#34;&gt;Khái niệm&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#c%C6%A1-ch%E1%BA%BF-ho%E1%BA%A1t-%C4%91%E1%BB%99ng-c%E1%BB%A7a-positional-encoding&#34;&gt;Cơ Chế Hoạt Động Của Positional Encoding&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#c%C3%A1c-ph%C6%B0%C6%A1ng-ph%C3%A1p-kh%C3%A1c&#34;&gt;Các Phương Pháp Khác&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#relative-positional-encoding&#34;&gt;Relative Positional Encoding&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#attention-with-linear-bias-alibi&#34;&gt;Attention with Linear Bias (ALiBi)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#rotary-positional-encoding-rope&#34;&gt;Rotary Positional Encoding (RoPE)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#t%C3%A0i-li%E1%BB%87u-tham-kh%E1%BA%A3o&#34;&gt;Tài Liệu Tham Khảo&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;style&gt;&#xA;  .img {&#xA;    display: block;&#xA;    margin-left: auto;&#xA;    margin-right: auto;&#xA;  }&#xA;  .imgTitle {&#xA;    text-align: center;&#xA;  }&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới Thiệu&lt;/h1&gt;&#xA;&lt;p&gt;Trong kiến trúc Transformer, trước khi Vector Embedding được đưa vào mô hình Encoder, nó được cộng thêm một vector khác để lưu trữ lại vị trí của các từ trong câu, cơ chế này gọi là Positional Encoding (PE).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear, Ridge and Lasso Regression</title>
      <link>http://localhost:1313/coffee/post/2023-06-10-linear-regression-1/</link>
      <pubDate>Sat, 10 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-10-linear-regression-1/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    margin-left: 10px;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;    display: block;&#xA;    margin-left: auto;&#xA;    margin-right: auto;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;p&gt;Như đã biết thì “học máy” (machine learning) là việc kết hợp các giả thuyết về dữ liệu và công cụ toán học để đưa ra thông tin, quy luật của dữ liệu.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tokenizers</title>
      <link>http://localhost:1313/coffee/post/2023-06-09-tokenizers/</link>
      <pubDate>Fri, 09 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-09-tokenizers/</guid>
      <description>&lt;style&gt;&#xA;    .singleImg {&#xA;        display: block;&#xA;        margin-left: auto;&#xA;        margin-right: auto;&#xA;    }&#xA;    .textSingleImg {&#xA;        text-align: center;&#xA;    }&#xA;    .twoImgBlock {&#xA;        display: flex;&#xA;        just-content: center;&#xA;    }&#xA;    .twoImg {&#xA;        width: 400px;&#xA;        height: 400px;&#xA;    }&#xA;&lt;/style&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#token-tokenizer-&#34;&gt;Token, Tokenizer ??&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#tokenization&#34;&gt;Tokenization&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#word-tokenization&#34;&gt;Word Tokenization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#character-tokenization&#34;&gt;Character Tokenization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#subword-tokenization&#34;&gt;SubWord tokenization&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#byte-pair-encoding-bpe&#34;&gt;Byte Pair Encoding (BPE)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#wordpiece-tokenization&#34;&gt;WordPiece tokenization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Có khá là nhiều bài viết nghiên cứu sâu về việc cách thức “máy móc” đọc văn bản kiểu gì ?&lt;/p&gt;&#xA;&lt;p&gt;Những bài viết nghiên cứu về ngôn ngữ và chuyên sâu về học thuật hơn khá nhiều, mình nghĩ nếu có thể thì các bạn có thể tìm đọc thêm.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dive To Entropy</title>
      <link>http://localhost:1313/coffee/post/2023-06-05-entropy_2/</link>
      <pubDate>Mon, 05 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-05-entropy_2/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới thiệu&lt;/h1&gt;&#xA;&lt;p&gt;Tiếp tục với Entropy, thì ở bài trước ta đã đi qua về định nghĩa Entropy trong machine learning cùng với một số ví dụ cũng như các tình huống cho việc Entropy cao, thấp.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Entropy In Machine Learning</title>
      <link>http://localhost:1313/coffee/post/2023-06-05-entropy_1/</link>
      <pubDate>Mon, 05 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-05-entropy_1/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;  text-align: center;&#xA;}&#xA;.textTwoImg {&#xA;    display: flex;&#xA;    flex-direction: row;&#xA;    justify-content: space-around;&#xA;&#xA;}&#xA;.singleImg {&#xA;  display: block;&#xA;  margin-left: auto;&#xA;  margin-right: auto;&#xA;}&#xA;.twoImg {&#xA;    display: inline;&#xA;    width: 300px;&#xA;    height: 300px;&#xA;    margin-left: 30px;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;h1 id=&#34;entropy-trong-machine-learning&#34;&gt;Entropy trong machine learning&lt;/h1&gt;&#xA;&lt;p&gt;Ở trong các lĩnh vực khác nhau, thì việc định nghĩa và áp dụng entropy cũng sẽ có thể khác nhau. Một khái niệm tổng quát về entropy, có thể nói ý tưởng của entropy là đo lường lượng thông tin.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SVMs Và Một Số Thứ Liên Quan</title>
      <link>http://localhost:1313/coffee/post/2023-06-04-svms-2/</link>
      <pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-04-svms-2/</guid>
      <description>&lt;style&gt;&#xA;.singleImg {&#xA;    display: block;&#xA;    margin-left: auto;&#xA;    margin-right: auto;&#xA;}&#xA;.textSingleImg {&#xA;    text-align: center;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;!-- @import &#34;[TOC]&#34; {cmd=&#34;toc&#34; depthFrom=1 depthTo=6 orderedList=false} --&gt;&#xA;&lt;!-- code_chunk_output --&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gi%E1%BB%9Bi-thi%E1%BB%87u&#34;&gt;Giới Thiệu&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#b%C3%A0i-to%C3%A1n-t%E1%BB%91i-%C6%B0u-cho-svm&#34;&gt;Bài Toán Tối Ưu Cho SVM&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#tham-s%E1%BB%91-tr%E1%BA%A3-v%E1%BB%81-t%E1%BB%AB-svc-trong-th%C6%B0-vi%E1%BB%87n-sklearn&#34;&gt;Tham số trả về từ SVC trong thư viện sklearn&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#t%C3%A0i-li%E1%BB%87u-tham-kh%E1%BA%A3o&#34;&gt;Tài Liệu Tham Khảo&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;!-- /code_chunk_output --&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới Thiệu&lt;/h1&gt;&#xA;&lt;p&gt;Đã bao giờ bạn chạy một thuật toán hay một thư viện nào đó và kết quả trả về là một đống thứ mà bạn không hiểu nó là gì chưa, hay khi tìm hiểu thuật toán có những thứ nhỏ trong thuật toán mà ta vô tình bỏ qua dẫn tới không hiểu các thứ đằng sau nó vận hành?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Support Vector Machine</title>
      <link>http://localhost:1313/coffee/post/2023-06-03-svms-1/</link>
      <pubDate>Sat, 03 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-03-svms-1/</guid>
      <description>&lt;style&gt;&#xA;    .singleImg {&#xA;        display: block;&#xA;        margin-left: auto;&#xA;        margin-right: auto;&#xA;    }&#xA;    .textSingleImg {&#xA;        text-align: center;&#xA;    }&#xA;    .twoImgBlock {&#xA;        display: flex;&#xA;        just-content: center;&#xA;    }&#xA;    .twoImg {&#xA;        width: 400px;&#xA;        height: 400px;&#xA;    }&#xA;&lt;/style&gt;&#xA;&lt;!-- code_chunk_output --&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gi%E1%BB%9Bi-thi%E1%BB%87u&#34;&gt;Giới thiệu&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#svms-l%C3%A0-g%C3%AC-&#34;&gt;SVMs là gì ?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#%C3%BD-t%C6%B0%E1%BB%9Fng-ch%C3%ADnh-c%E1%BB%A7a-svm&#34;&gt;Ý tưởng chính của SVM&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#soft-margin&#34;&gt;Soft margin&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#kernel&#34;&gt;Kernel&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#t%C3%A0i-li%E1%BB%87u-tham-kh%E1%BA%A3o&#34;&gt;Tài Liệu Tham Khảo&lt;br/&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;!-- /code_chunk_output --&gt;&#xA;&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới thiệu&lt;/h1&gt;&#xA;&lt;p&gt;SVMs (aka support vector machines) là một thuật toán quan trọng thường đường nhắc đến trong machine learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Loss, Cost and Objective Function</title>
      <link>http://localhost:1313/coffee/post/2023-06-01-loss-function/</link>
      <pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/post/2023-06-01-loss-function/</guid>
      <description>&lt;style&gt;&#xA;.textSingleImg {&#xA;    text-align: center;&#xA;}&#xA;&lt;/style&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#the-different-between-loss-cost-and-objective-function&#34;&gt;The different between loss, cost and objective function&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#some-note&#34;&gt;Some Note&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Các khái niệm xoay quanh loss, cost và object function là những thứ, mình tin chắc là bạn nên biết khi tìm hiểu về các kiến thức liên quan tới “machine learning”&lt;/p&gt;&#xA;&lt;p&gt;Cơ bản thì, loss function là 1 hàm số cho phép bạn đo lường sự sai khác giữa kết quả mà model dự đoán ra với kết quả thực. Với mong muốn thiết kế các model sao cho sự sai khác này là nhỏ nhất có thể (tức kết quả được dự đoán ra từ model giống với kết quả thực nhất có thể). Vậy, nếu sự sai khác này là lớn, hay kết quả được đoán ra từ model sai so với kết quả thực. Thì giá trị của loss function sẽ lớn (càng sai nhiều càng lớn) và ngược lại.&lt;/p&gt;</description>
    </item>
    <item>
      <title>About me</title>
      <link>http://localhost:1313/coffee/page/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/coffee/page/about/</guid>
      <description>&lt;p&gt;Mình là Đạt. Biệt danh của mình là Unique, nhưng mọi người thường gọi mình là &amp;ldquo;que&amp;rdquo; :v&lt;/p&gt;&#xA;&lt;p&gt;Mình cũng chả thích dài dòng lắm. Đây là nơi mình lưu lại những kiến thức của mình cũng như những thứ mà mình thấy thú vị xoay quanh công việc của mình.&lt;/p&gt;&#xA;&lt;p&gt;Mặc dù chủ yếu chỉ là nơi lưu lại những kiến thức của mình. Nhưng nếu nó có thể một phần nào đó giúp được các bạn thì mình rất vui.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
