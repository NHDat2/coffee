---
title: BERT
date: 2023-06-20
tags: ["Transformer", "Residual", "Feed Forward"]
---

<style>
.textSingleImg {
  text-align: center;
}
.textTwoImg {
    display: flex;
    flex-direction: row;
    justify-content: space-around;

}
.singleImg {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.twoImg {
    display: inline;
    width: 300px;
    height: 300px;
    margin-left: 30px;
}
</style>


# Mối liên hệ của Bert với Transformer

BERT, cũng là một trong số các pre-trained models dựa vào kiến trúc Transformer

Tuy nhiên, chỉ là kế thừa và dựa trên kiến trúc Transformer nên BERT cũng có những điểm khác biệt có thể kể đến như sau:

<img src="/img/bert/1.jpg" class="singleImg">

* BERT chỉ sử dụng “encoder”: với mục tiêu là tạo ra model có hiệu năng cao cho nhiều tác vụ khác nhau.

    Việc sử dụng phần kiến trúc encoder của Transformer, cho phép BERT encode thông tin về ngữ nghĩa và cú pháp trong quá trình embedding, điều mà cần thiết cho nhiều tác vụ khác nhau.

    * BERT không được thiết kế cho “text generation hay translations”, vì nó chỉ dùng encoder

    * BERT có thể train trên multiple languages. Tuy nhiên, bản thân nó không phải 1 model dịch máy.

    * Do vậy, thực tế nó có thể “predict token” nên nó cũng có thể sử dụng như một “text generation model” chỉ là nó không được tạo ra để tối ưu cho việc đó thôi.

<br/>

* BERT không sử dụng “decoder”: như đã đề cập ở trên, BERT chỉ sử dụng phần “encoder” của Transformer mà không sử dụng “decoder”.\
    Khi đó, output của BERT sẽ là 1 embedding chứ không phải dạng văn bản.
    * Do vậy, khi sử dụng BERT thì ta luôn cần phải làm thêm 1 thứ gì đó đằng sau nó, có thể chỉ đơn giản là tính “similarity score” hoặc làm bất cứ thứ gì với “embedding” thu được phục vụ cho mục đích của chúng ta.

    * Ngược lại, với kiến trúc Transformer có phần decoder, output của nó sẽ ở dạng văn bản và chúng ta có thể cầm output đi sử dụng luôn được.

# Tổng quan về BERT

Về cơ bản, ý tưởng của BERT là sử dụng learning objective masking để che đi 1 hoặc 1 phần token trong câu đầu vào, và bắt model phải dựa vào các tài nguyên sẵn có, là các token xung quanh các token bị “che” để có thể tái cấu trúc và dự đoán lại token bị “che” đó.

# Tokenizers

Tokenizer góp phần quan trọng trong việc encode để khiến một mô hình xử lý ngôn ngữ tự nhiên có thể hiểu được văn bản đầu vào, đối với BERT cũng không ngoại lệ.

Từ trước tới nay đã có rất nhiều phương pháp, cách tiếp cận để có thể tách từ giúp việc encode đầu vào trở lên dễ dàng hơn có thể kể đến như:

* Tách từ bằng một số rules

* Tách từ bằng các dấu câu

* Đơn giản nhất là tách từ bằng dấu khoảng trắng

* Thậm trí là xây dựng một mô hình dành riêng cho việc tách từ, câu..v.v

(Mình đã có một bài nói về việc Tokenizers, mọi người có thể xem chi tiết hơn).

Đối với BERT, BERT sử dụng một tokenizer có tên là “WordPiece”. Đây là một thuật toán cho “subword tokenization”.

Với “WordPiece” BERT chỉ cần sử dụng bộ từ điển với 30,000 tokens đã có thể cover được hết toàn bộ từ và không cần phải dùng tới "UNK" token

<img src="/img/bert/2.jpg" class="singleImg">
<p class="textSingleImg"><b>Hinh 2: WordPiece tokenization</b></p>

# Masked Language Model

Masked Language Models kiểu dạng như BERT, sử dụng một cách tiếp cận gọi là “masking”, khi đó model sẽ cố gắng học để dự đoán 1 token bị che (masked) ngẫu nhiên trong câu đầu vào.

Điều này đòi hỏi model phải nhìn được toàn bộ văn bản để có thể hiểu được ngữ cảnh xung quanh và dự đoán token bị che đó.

Do vậy, MLMs sẽ xem xét toàn bộ câu đầu vào theo cả 2 chiều thay vì chỉ 1 chiều như các LMs khác.

Như đã đề cập trước đó, BERT chỉ sử dụng Encoder lấy cảm hứng từ kiến trúc Transformer, khi đó đầu ra của BERT sẽ là một embedding

Do vậy để có thể học được thì cần phải gán vào sau đó 1 hoặc nhiều tác vụ để giúp model có thể học (learning objective), và Masking là một learning objective đưa BERT trở thành model out-performance lúc bấy giờ.

Masking là một thành phần quan trọng và làm BERT thành công cũng như khác biệt với các models khác, nhưng nó cũng được xây dựng dựa trên cơ chế attention được giới thiệu ngay trong kiến trúc Transformer ban đầu.

<img src="/img/bert/3.jpg" class="singleImg">

## Cách thức Masking

Ta có thể chia cách masking trong BERT gồm 2 phần:

### 15% input tokens

Mask 15% input tokens: BERT sẽ không chỉ masking (che) 1 token bất kỳ. Thay vào đó, nó sẽ ngẫu nhiên chọn 15% lượng token trong câu đầu vào và thực hiện che đi.

À thì đương nhiên rồi =)), con số 15% này cũng được đưa ra thông qua phương pháp “thử và sai”.

	1. > 15%: Nếu số lượng token bị che đi > 15% thì sẽ bắt đầu khiến cho model gặp khó khăn trong việc học từ đầu vào vì lượng dữ liệu bị che đi khá nhiều.

	2. < 15%: Nếu lượng token che đi < 15%, thì model sẽ mất nhiều thời gian và sẽ cần nhiều dữ liệu hơn để có thể hiểu được ngữ cảnh một cách tổng quát hơn, vì khi số lượng token bị che ít thì cũng khiến model dễ dàng hơn trong việc dự đoán các token bị che đó.

### Mask, Correct, Wrong token

Số lượng token được “che” đã được quyết định ở trên, tiếp đến là việc sẽ “che” token đó như thế nào. Trong bài báo gốc, các tác giả thực hiện “che” các token bằng một số kiểu khác nhau như:

    * Mask token: Thay thế token cần được “che” bằng token [MASK]
    * Wrong token: Thay thế token cần được “che” bằng 1 token khác
    * Correct token: Giữ nguyên token cần được “che” mà không làm gì cả

<br/>

Với một tỉ lệ được chia như sau, Trong 15% token cần được “che”:

    1. Mask token: chiếm 80% (trong tổng số 15%), có nghĩa, ta sẽ thực hiện thay thế 80% token cần được “che” trong số tổng 15% token cần được “che” thành [Mask] token.

<img src="/img/bert/4.jpg" class="singleImg">
<p class="textSingleImg"><b>Hinh 4: Mask Token</b></p>
<br/>

    2. Wrong token: chiếm 10% (trong tổng số 15%), ta sẽ thực hiện thay thế 10% token cần được “che” trong số tổng 15% thành một token khác.

<img src="/img/bert/5.jpg" class="singleImg">
<p class="textSingleImg"><b>Hinh 5: Wrong Token</b></p>
<br/>

    3. Correct token: chiếm 10% (trong tổng 15%), ta sẽ thực hiện giữ nguyên 10% token cần được “che” trong số tổng 15% mà không làm gì cả.

<img src="/img/bert/6.jpg" class="singleImg">
<p class="textSingleImg"><b>Hinh 6: Correct Token</b></p>


Nhóm tác giả, có lý giải tại sao lại chia việc “masking” thành 3 loại khác nhau, tuy nhiên các con số 80,10,10 cũng chỉ được nhóm tác giả đưa ra thông qua phép thử và sai.

    1. Chỉ sử dụng [Mask] token: Nếu chỉ sử dụng [Mask] token thôi, thì sau model chỉ học dựa vào một ít ngữ cảnh xung quanh. Khi đó, model có thể sẽ bỏ quên rất nhiều thông tin từ các ngữ cảnh xung quanh mà chỉ tập trung vào target token là [Mask] token để chăm chăm dự đoán xem token đó là gì.

    2. Sử dụng [Mask] và Correct token: giả dụ nếu sử dụng 80% cho [Mask] token và 20% cho Correct token. Thì khi đó, model sẽ học được 1 tín hiệu để hiểu rằng à, nếu không có [Mask] token tức là toàn bộ token đều đúng, và khi đó model sẽ không học được gì ở đấy cả

    3. Sử dụng [Mask], Correct, Wrong token: nếu ta thêm các trường hợp thay thế token cần “che” thành cả “wrong token” (các token khác token ban đầu), thì từ đó model sẽ luôn học và cố gắng tìm hiểu để học được từ ngữ cảnh rằng token nào cần phải sửa lại.

# Thảo luận

Tuy nhiên đấy chỉ là các thông tin cũng như đánh giá của các tác giả trong bài báo gốc của BERT.

Thực thế, có một số bài báo sau đó đã được ra đời và cho thấy rằng chúng ta cũng vẫn chưa thực sự hiểu rõ hết về các khía cạnh trong objective learning MLM này.

Cụ thể như sau:

Masked Language Modeling and the Distributional Hypothesis: trong paper này, các tác giả đã chỉ ra rằng “thứ tự” của các từ không quá quan trọng trong MLMs mà chỉ ảnh hưởng 1 phần nhỏ tới hiệu suất.

Bằng cách, thực hiện xáo trộn ngẫu nhiên thứ tự các từ ở các câu đầu vào, và thực hiện các thử nghiệm.  Các tác giả của bài báo cho rằng lý do mà MLMs cải thiện được hiệu suất là vì:

	* việc thực hiện đánh giá các tác vụ NLP còn khá dễ đối với các dạng Large pre-Trained Language Model , điều này khiến cho việc đánh giá các dạng model này còn chưa thực sự chính xác.
	* nhóm tác giả cho rằng việc MLMs tốt phần lớn là vì MLM có khả năng lập mô hình thống kê từ cùng xuất hiện ở các bậc cao hơn và masking, attention cho phép BERT tìm hiểu thêm thông tin so với word2vec
