---
title: Tản Mạn Pre-Trained Models
date: 2023-06-17
tags: ["transformer", "pre-trained model"]
---


<style>
.textSingleImg {
  text-align: center;
}
.textTwoImg {
    display: flex;
    flex-direction: row;
    justify-content: space-around;

}
.singleImg {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.twoImg {
    display: inline;
    width: 300px;
    height: 300px;
    margin-left: 30px;
}
</style>

# Transformer-based Pre-trained Models là gì ?

T-PTMs là các model lấy kiến trúc Transformer đình đám được công bố vào năm 2018 làm nền tảng phát triển với các biến thể khác nhau cùng với lượng cực lớn dữ liệu để giúp model có thể nắm bắt được 1 khối kiến thức đồ sộ.

Các T-PTMs có thể nắm bắt hiệu quả các kiến thức bằng cách lưu chúng vào lượng lớn các parameter và fine-tune chúng trên các tác vụ cụ thể.

<img src="/img/transformer_base_preTrain/1.jpg" class="singleImg">

Các T-PTMs tận dụng hiệu quả của self-supervised learning cùng với đó là lượng dữ liệu không nhãn “unlabeled data”. Từ đó, thiết kế ra các pre-Trained tasks để huấn luyện model giúp model có được 1 lượng kiến thức khổng lồ.

Đó là 1 cái nhìn chung chung thôi, đương nhiên là sẽ còn nhiều điểm thêm thắt vào tuỳ thuộc cách tiếp cận của các tác giả nữa (ta sẽ đi cụ thể ở các bài khác nói riêng về các models đó sau). Ví dụ, như họ GPT cũng sử dụng thêm cả dữ liệu đã gán nhãn “labeled data” để huấn luyện, ..v.v

# Nhìn lại 1 chút quá trình phát triển của Pre-Trained Models

## Deep Learning

<img src="/img/transformer_base_preTrain/2.jpg" class="singleImg">

Có thể thấy rằng, khi deeplearning bắt đầu được khai phá và thời kỳ deeplearning được bắt đầu.

Nhà nhà deeplearning, người người deeplearning, thì deeplearning cũng đã cho thấy sức mạnh của mình trong các tác vụ về AI cả về Computer Vision hay NLP ..v.v.

Tuy nhiên, sau một thời gian phát triển các nhà nghiên cứu nhận thấy rằng đâu đó deep neural network vẫn tồn đọng khá nhiều thử thách và một trong số đó là việc “Đói Dữ Liệu”.

Vì Deep neural network thường có lượng lớn tham số, do vậy, sẽ dẫn đến việc model bị overfit và thiếu sự tổng quát hoá nếu như không có đủ dữ liệu để training.

Trong khi đó, nỗ lực để tạo ra dữ liệu cho việc train các mô hình deep neural network rất đắt đỏ, về cả công sức, tiền bạc lẫn thời gian.

Do vậy, trong 1 khoảng thời gian dài có một “vấn đề” được tìm kiếm rất nhiều theo kiểu là “làm sao để huấn luyện mô hình deep neural network hiệu quả với tài nguyên hạn chế…v.v..”

## Transfer Learning

<img src="/img/transformer_base_preTrain/3.jpg" class="singleImg">

Một trong những cột mốc quan trọng xuất hiện trong quá trình giải quyết vấn đề nan giải ở trên là cụm từ “Transfer Learning”.

Thực tế con người có thể sử dụng các kiến thức, kinh nghiệm (đã gặp, đã học, đã trải qua…v.v.) trước đó để giải quyết các vấn đề mới. Lấy cảm hứng từ đó, Transfer Learning tiếp cận dựa trên 2 giai đoạn:

* Giai đoạn đầu, pre-trained phrase để nắm bắt được 1 lượng kiến thức nhất định dựa trên 1 hoặc nhiều tác vụ nguồn khác nhau
* Fine-tune phrase, sau đó transfer (chuyển giao) kiến thức tới tác vụ đích

Do đã có nhiều kiến thức thu được từ các tác vụ nguồn trong giai đoạn pre-training, nên trong giai đoạn fine-tune model có thể sử lý tốt hơn các tác vụ đích với số lượng mẫu có giới hạn.

Thời điểm đó, transfer learning được áp dụng khá rộng rãi, khi phần nào giải quyết được các nhược điểm của việc “Đói dữ liệu” gây ra.

Đặc biệt là trong computer vision, khi mà hàng loạt các nghiên cứu, các biến thể của CNNs được áp dụng dựa trên việc pre-Trained tập dữ liệu “human annotated visual recognition dataset ImageNet”.

Khi mà tập dữ liệu “human-annotated visual recognition dataset ImageNet” với lượng dữ liệu lớn và đa dạng xuất hiện, khiến cho model CNN được pre-trained trên ImageNet hoạt động rất tốt khi fine-tune chỉ với lượng nhỏ dữ liệu cho các down-stream task.

Điều này cũng có thể coi là cú huých đầu tiên cho làn sóng pre-trained models (PTMs) trong kỷ nguyên deep learning.

## Transformer-based Pre-Trained models

Theo làn sóng pre-trained models, cộng đồng NLP cũng nhập cuộc.

Một cột mốc cực kỳ quan trọng năm 2017, thời điểm mà Transformer được công bố, một dạng kiến trúc có khả năng train được các mạng cực sâu.

Để tận dụng được hết kho ngữ liệu chưa được gán nhãn cực lớn trong mảng NLP, self-supervised learning được đề xuất, bằng việc lấy 1 phần bản thân làm nhãn và phần còn lại để train thì SSL đã phần nào giúp cho cộng đồng NLP thành công trong việc tận dụng kho ngữ liệu đồ sộ mà chưa được gán nhãn.

Từ thời điểm đó đến nay, đã có hàng loạt các pre-trained model được tạo ra dựa trên kiến trúc Transformer và các biến thể với cách tiếp cận Self-Supervised Learning.

<img src="/img/transformer_base_preTrain/4.jpg" class="singleImg">

Các kiến trúc trên chỉ mang tính chất ví dụ, lấy mẫu và tham khảo tính tới thời điểm của bài viết này. Còn rất nhiều kiến trúc khác nữa mà chưa liệt kê trong này.

Tuy nhiên, Transformer-based Pre-Trained Models vẫn còn tồn đọng một số hạn chế như:

* Bản chất ẩn đằng sau lượng khổng lồ các tham số của mô hình vẫn chưa thực sự quá rõ ràng đối với chúng ta.
* Với lượng lớn tham số và lượng lớn dữ liệu thì chi phí để đào tạo các mô hình này vẫn còn là một rào cản lớn.
