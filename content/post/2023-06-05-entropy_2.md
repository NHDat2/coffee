---
title: Dive To Entropy
date: 2023-06-05
tags: ["entropy formula", "entropy"]
---


<style>
.textSingleImg {
  text-align: center;
}
.textTwoImg {
    display: flex;
    flex-direction: row;
    justify-content: space-around;

}
.singleImg {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.twoImg {
    display: inline;
    width: 300px;
    height: 300px;
    margin-left: 30px;
}
</style>



# Giới thiệu

Tiếp tục với Entropy, thì ở bài trước ta đã đi qua về định nghĩa Entropy trong machine learning cùng với một số ví dụ cũng như các tình huống cho việc Entropy cao, thấp.

Trong bài này, ta sẽ đi sâu hơn nữa vào công thức của Entropy trong machine learning

• Nó gồm các thành phần gì ?\
• Đại diện cho cái gì ?\
• Sự hình thành của công thức Entropy ?

# Dive To Entropy Formula

Shanon định nghĩa công thức Entropy có dạng:

\\[ H = - \sum p(x) * log_2(p(x)) \\]

Ta sẽ đi qua các khái niệm liên quan cũng như sự hình thành của công thức

## Surprise

Cũng là ví dụ giống với bài trước đó, về “bi trong túi”.

Case 1: trong túi có 4 bi đỏ, 1 bi xanh

Case 2: trong túi có 9 bi đỏ, 1 bi xanh

Case 3: trong túi có 5 bi đỏ, 5 bi xanh

<img src="/img/entropy/3_bag.jpg">

Trong ví dụ trên, ta có thể thấy:

Trong Case 1, khi tỉ lệ chênh lệch không nhiều thì, việc bốc được bi đỏ hay bi xanh thì cũng có ngạc nhiên đó, nhưng cũng không nhiều lắm

Trong Case 2, Khi tỉ lệ số lượng bi các màu lệch nhiều, thì việc bốc được bi đỏ thì chả có tí ngạc nhiên nào, vì nhiều bi đỏ thế thì bốc được cũng thường thôi. Nhưng ngược lại khi bốc được bi xanh thì nó cứ phải gọi là “ảo ma canada lazada malzahar bunhia-copxki” luôn ý chứ :v.

Trong Case 3, Khi tỉ lệ bằng nhau, thì việc bốc được bi đỏ hay bi xanh thì mức độ ngạc nhiên như nhau.

**Do vậy, ta có thể thấy, có vẻ như “mức độ ngạc nhiên” (surprise) sẽ tỉ lên nghịch với xác suất.**

**- Nếu xác suất xảy ra sự kiện X càng thấp, thì khi xảy ra sự kiện X thì ”ngạc nhiên” càng nhiều.**

**- Ngược lại, nếu xác suất xảy ra sự kiện X càng cao, thì khi xảy ra sự kiện thì “ngạc nhiên” càng ít**

**- Và nếu 2 sự kiện có xác suất xảy ra như nhau, thì sự “ngạc nhiên” khi xảy ra 1 trong 2 sự kiện là như nhau**

Khi đó, Ta có công thức tính mức độ ngạc nhiên như sau:

\\[ Surprise = {1 \over probability} \\]

**Cứ tưởng vậy là OK**. Tuy nhiên, lại có một số vấn đề với công thức ở trên mà trong đó có 1 vấn đề ảnh hưởng trực tiếp tới mối quan hệ giữa “surprise” và “probability”. Ta sẽ xem xét lại dưới đây.

Ví dụ, ta thực hiện thử nghiệm “tung đồng xu”. Ta cứ thế mà tung, lần 1, lần 2….v.v., và lần nào cũng ra “mặt hình” đến lần thứ 80, 81, 82 ồ vẫn ra mặt hình sao “lạ nhể” và cứ thế tung tiếp, và cứ ra mặt hình tiếp, chán lắm rồi, lúc ý kiểu tung cho xong để đạt chỉ tiêu ý, lần thứ 198, 199 vẫn mặt hình..ok, bạn tung tiếp.

Thì câu hỏi đặt ra là tiếp theo nếu nó ra mặt hình tiếp thì bạn có bất ngờ nữa không ?

Thì có vẻ như là cũng không còn bất ngờ nữa đúng không. Đúng kiểu cái gì nhiều quá cũng nhàm :v

Khi đó, nếu ta chỉ tung 200 lần:

<img src="/img/entropy/cal_1.jpg">
<img src="/img/entropy/plot_1.jpg">
<p class="textSingleImg"><b>Hình: Đồ thị hàm f(x) = 1/x, all x in [0, 1]</b></p>

Có thể thấy, khi p = 1 => s = 1. Mà trong khi đó, thực tế, thì ta chẳng ngạc nhiên gì cả

Do vậy, công thức trên chưa hẳn thực sự phản ánh được mối quan hệ giữa 2 đại lượng “surprise” và “probability”.

Để giải quyết vấn đề này, ta thực hiện thêm hàm Log vào hàm số trên. Khi đó:

<img src="/img/entropy/cal_2.jpg">
<img src="/img/entropy/plot_2.jpg">
<p class="textSingleImg"><b>Hình: Đồ thị hàm f(x) = log(1/x), all x in [0, 1]</b></p>

**NOTE: Các giá trị undefined ở trên, ta có thể chấp nhận được với việc p -> ~0 (\\( 10^{-n} \\), 1 số cực nhỏ tiến gần tới 0, thì vẫn có thể xác định được các biểu thức trên)**

Khi đó, nhìn vào biểu đồ Hình 2, của hàm số \\( S = log_2({1 \over p}) \\), có vẻ phù hợp để mô tả mối quan hệ của “surprise” và “probability” hơn rồi.

Do vậy, Biểu thức biểu diễn mối quan hệ giữa “Surprise” và “Probability” là:

\\[ S = log_2({1 \over p}) \\]

Ví dụ, giả sử ta có 1 thí nghiệm tung đồng xu, và xác suất tung ra “mặt hình” là 0.8, xác suất tung ra “mặt chữ” là 0.2.

<img src="/img/entropy/cal_3.jpg">

Ta có thể thấy, với xác suất tung ra các mặt như trên, thì tung được ra mặt chữ, sẽ bất ngờ nhiều hơn so với mặt hình

## Surprise to Entropy

Ok vẫn dùng ví dụ ở trên, nhưng ta thực hiện tung 4 lần liên tiếp. Ta nhận được kết quả theo thứ tự là “hình, hình, chữ, hình”

<img src="/img/entropy/4_flipping.jpg">

Khi đó, xác suất để trường hợp tung 4 lần liên tiếp với kết quả trên xảy ra sẽ là “0.8*0.8*0.2*0.8” (vì nó là sự xảy ra đồng thời, nên dùng joint probability)

=> Mức độ “ngạc nhiên” khi trường hợp trên xảy ra sẽ là:

\\[ s = log_2({1 \over 0.8 * 0.8 * 0.2 * 0.8}) \\]
\\[ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~= log_2({1 \over 0.8}) + log_2({1 \over 0.8}) + log_2({1 \over 0.2}) + log_2({1 \over 0.8}) \\]
\\[ ~~~~~~~= 0.32 + 0.32 + 2.32 + 0.32 \\]
\\[ = 3.28 \\]

**Note: Có thể thấy, mức độ “ngạc nhiên” của toàn bộ các lần tung đồng xu, sẽ bằng tổng “ngạc nhiên” của từng lần tung đơn lẻ.**

Ok, thế giờ ta thực hiện tung đồng xu 200 lần thì sao ? Mức độ “ngạc nhiên” của việc tung đồng xu 200 lần liên tiếp sẽ như thế nào ?

Có thể thấy, ở ví dụ trước đó, ta đã biết 3 lần ra “mặt hình” và 1 lần ra “mặt chữ”. Sau đó, mức độ “ngạc nhiên” cho 4 lần tung liên tiếp đó được ước tính bằng 3 lần “ngạc nhiên” khi ra “mặt hình” cộng với 1 lần “ngạc nhiên” khi ra “mặt chữ”.

Do vậy, với n = 200 lần tung đồng xu. Để ước lượng được mức độ “ngạc nhiên” của việc tung n lần như vậy, ta cần ước lượng được khi tung n lần như vậy thì bao nhiêu lần ra “mặt hình”, bao nhiêu lần “mặt chữ”. Khi đó ta có thể ước lượng theo công thức.

\\[ s = (p_{hình} * n) * s_{hình} + (p_{chữ} * n) * s_{chữ} \\]
\\[ = (0.8 * 200) * 0.32 + (0.2 * 200) * 2.32 \\]
\\[ = 144 \\]

Ok, giờ ta đã tính được mức độ “ngạc nhiên” khi tung đồng xu n = 200 lần liên tiếp.

Vậy giờ nếu ta chia toàn bộ cho n => ta sẽ có được mức độ “ngạc nhiên” trung bình của mỗi lần tung xu.

\\[ E(s) = {(p_{hình} * n) * s_{hình} + (p_{chữ} * n) * s_{chữ} \over n} \\]
\\[ = 0.8 * 0.32 + 0.2 * 2.32 \\]
\\[ = 0.72 \\]

Khi đó, ta ước lượng rằng trung bình mỗi lần tung đồng xu thì mức độ “ngạc nhiên” sẽ là 0.72. **Và đây chính là Entropy của đồng xu**.

Khi đó, ta có:

\\[ H(x) = E(s) = \sum x * P(X=x) \ \ (1) \\]

Trong đó:

* **x**: mức độ “ngạc nhiên” ứng với sự kiện x

* **P(X=x)**: xác suất xảy ra của sự kiện x

(1) có thể biến đổi theo dạng:

\\[ H(x) = E(s) = \sum log_2({1 \over p(x)}) * p(x) \\]
\\[ = - \sum log_2(p(x)) * p(x) \\]

=> ta có công thức Entropy theo dạng

\\[ H(x) = - \sum p(x) * log_2(p(x)) \\]

# Entropy, Example

Quay trở lại ví dụ bốc bi ban đầu.

<img src="/img/entropy/3_bag.jpg">

Áp dụng công thức tính Entropy:  cong thuc

Case 1: \\( H(x) = 0.8\*log_2({1 \over 0.8}) + 0.2\*log_2({1 \over 0.2}) = 0.72 \\)

Case 2: \\( H(x) = 0.9\*log_2({1 \over 0.9}) + 0.1\*log_2({1 \over 0.1}) = 0.47 \\)

Case 3: \\( H(x) = 0.5\*log_2({1 \over 0.5}) + 0.5\*log_2({1 \over 0.5}) = 1 \\)

Ta có thể thấy

* Ở Case 1 thì khả năng bốc được bi xanh vẫn cao hơn là so với Case 2. Do vậy, việc độ “ngạc nhiên”, Entropy của Case 2 cao hơn Case 1, thì cũng là điều hợp lý thôi.

* Entropy càng thấp, thì lượng chênh lệch số bi giữa các màu trong túi càng lớn. Hay lượng bi giữa các màu càng ngang nhau thì Entropy càng lớn và lớn nhất sẽ bằng 1 khi số lượng bi giữa các màu bằng nhau ( hay xác suất để bốc được bi xanh hay đỏ là như nhau, phân phối đều)

# Maximum Entropy

Trong phần này ta sẽ đi tìm
* “Giá trị lớn nhất” mà Entropy có thể đạt được ?
* Đạt được nó khi nào ?
* Sao mà lại đạt được nó ?

Công thức Entropy có dạng :

\\[ H(x) = - \sum p(x) * log_2(p(x)) \\]

Đặt \\( p(x) = p \\) (để ở dưới biến đổi cho đỡ rối mắt)

Ta gọi:

\\[ f(p) = log_2(p)\ \,\ p \in \[0\,1\] \\]

Ta có:

\\[ f^'(p) = {1 \over pln(2)}\ \,\ mà\ ({1 \over x})^' = {-x^' \over x^2} \\]
\\[ => f^"(p) = {-1 \over p^2ln(2)} < 0\ \,\ \forall\ \ p \in \[0\,1\]\ \ \(*\) \\]

Từ (*) => \\( f(p) = log_2(p) \\) là hàm **lõm** (Concave function)

Ta xét hàm số:

\\[ f(p) = log_2(p) \\]

vì \\( f(p) \\) là hàm **lõm**

=> Áp dụng **bất đẳng thức Jensen** ta có:

\\[ E(f(x)) \le f(E(p))\ \ \Leftrightarrow\ \ p_1 = p_2 = p_3 = ... = p_n \\]
\\[ \Leftrightarrow\ \ E(log_2(p)) \le log_2(E(p))\ \ với\ \ E(p) = {p_1 + p_2 + ... + p_3 \over n} = {1 \over n} \\]
\\[ \Leftrightarrow\ \sum^{n}p_ilog_2(p_i) \le log_2({1 \over n}) \\]
\\[ \Leftrightarrow\ -\sum^{n}p_ilog_2(p_i) \le log_2(n) \\]
\\[ \Rightarrow \max (H) = log_2(n) \\]

Do vậy, \\( \max (H) = log_2(n)\ \ \Leftrightarrow\ \ p_1 = p_2 = ... = p_n \\) hay nói cách khác p phải là phân phối đều (**Uniform distribution**)

Quay trở lại ví dụ ở trên, có thể thấy chỉ có case 3 là tuân theo phân phối đều (uniform distribution). Thì đó là lý do vì sao case 3 có Entropy cao hơn case 1 và case 2.

Một hình đồ thị mà, ta có thể bắt gặp nhiều là đồ thị Entropy cho phân phối Bernouli. Với phân phối Bernouli mỗi biến X có thể nhận 1 trong 2 giá trị (X = {0, 1})

<img src="/img/entropy/plot_3.jpg">

<p class="textSingleImg"><b>Hình: Đồ thị giá trị Entropy cho phân phối Bernouli khi (X = {0, 1})</b></p>

Phân phối này được dùng nhiều trong thuật toán "Decision Tree" với triển khai cây với 2 nhánh, và khi đó \\( \max H(X) = 1\ \ \Leftrightarrow\ \ Bernouli\ Distribution \\).

