---
title: Entropy In Machine Learning
date: 2023-06-05
tags: ["entropy formula", "entropy"]
---


<style>
.textSingleImg {
  text-align: center;
}
.textTwoImg {
    display: flex;
    flex-direction: row;
    justify-content: space-around;

}
.singleImg {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.twoImg {
    display: inline;
    width: 300px;
    height: 300px;
    margin-left: 30px;
}
</style>



# Entropy trong machine learning

Ở trong các lĩnh vực khác nhau, thì việc định nghĩa và áp dụng entropy cũng sẽ có thể khác nhau. Một khái niệm tổng quát về entropy, có thể nói ý tưởng của entropy là đo lường lượng thông tin.

Trong machine learning, Entropy có thể gọi là mức độ không chắc chắn hoặc độ hỗn loạn cũng đều hợp lý.

**Case 1**: Ví dụ, cho 7 viên bi “đỏ vàng cam lục lam tràm tím”, bốc ngẫu nhiên 1 viên bi trong đống này. Thì khi đó entropy sẽ cao, vì mức độ chắc chắn để bốc được 1 viên bi theo ý muốn là rất thấp, nếu vissualize ra thì sẽ là 7 chấm khác màu nhau (cũng có thể gọi là hỗn loạn). **(Hình 1)**

**Case 2**: Hoặc nhưng nếu trong túi có 7 viên bi mà 6 viên đỏ và 1 viên vàng. Thì xác suất để bốc ra được viên bi đỏ lại rất cao => Khi đó, entropy sẽ thấp, vì ta có thể chắc chắn phần lớn khi bốc bi sẽ bốc ra được bi màu đỏ là rất cao. **(Hình 2)**

<img class="twoImg" src="/img/entropy/1.jpg">
<img class="twoImg" src="/img/entropy/2.jpg"><br/>

<div class="textTwoImg">
    <p>Hình 1</p>
    <p>Hình 2</p>
</div>

**Case 3**: Một ví dụ khác, nếu trong điều kiện lý tưởng khi tung xúc xắc. Thì việc tung được 1 mặt theo ý muốn (mặt 1 chấm, mặt 2 chấm, ..v.v) cũng là không chắc chắn hay entropy cao. Vì xác suất để tung ra 6 mặt là = nhau. **(Hình 3)**

**Case 4**: Nhưng thay vì 1 con xúc xắc ở điều kiện lý tưởng, nếu ta gian lận và tung 1 con xúc xắc với toàn bộ 6 mặt là 6 chấm. Thì chả cần phải tính hay đoán ta cũng chắc chắn rằng nó sẽ ra mặt 6 chấm => khi đó entropy thấp. **(Hình 4)**

<img class="twoImg" src="/img/entropy/3.jpg">
<img class="twoImg" src="/img/entropy/4.jpg"><br/>

<div class="textTwoImg">
    <p>Hình 3</p>
    <p>Hình 4</p>
</div>

Với shanon, Entropy được định nghĩa theo công thức sau:

\\[ H = -\sum^{n}p_ilog_2(p_i) \\]

# Thông tin, Entropy

Theo những gì mình research được thì, Entropy được giới thiệu bởi Shanon là vay mượn của “vật lý” và áp dụng vào lý thuyết thông tin.

Giả sử, trong 1 trò chơi của Long, Tuấn và đám bạn. Long sẽ đứng với đám bạn ở đầu này tung lần lượt 2 lần 1 đồng xu và viết “kết quả” vào “tin nhắn” gửi cho Tuấn.

Và Tuấn ở đầu bên kia sẽ đoán mặt đồng xu được tung ra theo các lần mà Long đã tung rồi so xem có đúng với kết quả Long ghi không. Với việc gửi “tin nhắn” bằng binary code: 0 là mặt Hình, 1 là mặt chữ.

**Case 1**: Nếu sử dụng 1 đồng xu mà cả 2 mặt là “mặt hình”. Sau khi Long tung đồng xu 2 lần và gửi tin nhắn. Thì Tuấn cũng đã chắn chắn được luôn là cả 2 lần đều là mặt hình. Khi đó, ở đây thông tin mà Long cần truyền tải cho Tuấn = 0, vì kiểu gì đáp án cũng chỉ có vậy và Tuấn có thể chắc chắn về đáp án.

**Case 2**: Dùng 1 đồng xu bình thường, khi đó các tin nhắn mà Long gửi cho Tuấn có thể là “00, 01, 10, 11”, và mỗi tin nhắn này có thể biểu diễn bằng 2 bits.

Ở đây, có thể thấy, Trong case 1, Tuấn có thể chắc chắn 100% kết quả là gì thậm trí chả cần đọc tin nhắn mà Long gửi.

Trong khi đó, ở case 2, Tuấn chỉ có thể đoán 1 trong 4 trường hợp kết quả, cái nào mới là cái chính xác. Khi đó sự chắc chắn của Tuấn chỉ còn 25%, vì nhiều thông tin (trường hợp kết quả có thể xảy ra) quá nên chả biết cái nào mới là cái đúng.

**Một cách tổng quát thì, càng nhiều thông tin được truyền tải => độ chắc chắn về thông tin càng thấp, càng rối loạn.**

**Hay, càng biết ít về “tin nhắn” định nói gì thì càng cần nhiều “thông tin” để truyền tải.**


